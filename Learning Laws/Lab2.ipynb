{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "* **Name**: Utkarsh Prakash\n",
    "\n",
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Title**: Demonstrate the realization of AND gate and NAND gate using different learning laws for a neuron.\n",
    "\n",
    "**Objective**: To observe and understand the working of different learning laws for a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-1: Hebbian Learning Law\n",
    "#### Hypothesis: \n",
    "The Hebbian Learning Law should be able to learn appropriate weights to implement the AND gate and NAND gate. <br />\n",
    "\n",
    "#### Truth Tables:\n",
    "\n",
    "**AND Gate** <br />\n",
    "\n",
    "| $X$ | $Y$ | $Output$ | \n",
    "| --- | --- | --- |\n",
    "| $0$ | $0$ | $0$ |\n",
    "| $0$ | $1$ | $0$ |\n",
    "| $1$ | $0$ | $0$ |\n",
    "| $1$ | $1$ | $1$ |\n",
    "\n",
    "**NAND Gate** <br />\n",
    "\n",
    "| $X$ | $Y$ | $Output$ | \n",
    "| --- | --- | --- |\n",
    "| $0$ | $0$ | $1$ |\n",
    "| $0$ | $1$ | $1$ |\n",
    "| $1$ | $0$ | $1$ |\n",
    "| $1$ | $1$ | $0$ |\n",
    "\n",
    "#### Experimental Description:\n",
    "1.   Data generation: For training purposes we will consider all the possible combinations of two inputs to the gate along with their desired output i.e. the truth table will be our training data. However, the values of $0$ in the output will be replaced by $-1$. <br />\n",
    "2.  Objective: We should be able to realize the following:<br />\n",
    "\n",
    "**AND Gate** <br />\n",
    "\n",
    "| $a_1$ | $a_2$ | $b$ | \n",
    "| --- | --- | --- |\n",
    "| $-1$ | $-1$ | $-1$ |\n",
    "| $-1$ | $1$ | $-1$ |\n",
    "| $1$ | $-1$ | $-1$ |\n",
    "| $1$ | $1$ | $1$ |\n",
    "\n",
    "**NAND Gate** <br />\n",
    "\n",
    "| $a_1$ | $a_2$ | $b$ | \n",
    "| --- | --- | --- |\n",
    "| $-1$ | $-1$ | $1$ |\n",
    "| $-1$ | $1$ | $1$ |\n",
    "| $1$ | $-1$ | $1$ |\n",
    "| $1$ | $1$ | $-1$ |\n",
    "\n",
    "3. Operations: <br />\n",
    "   The change in the weight vector is given by:\n",
    "   <center> $\\Delta w_i = \\eta f(w^T_i a)a$ </center> <br />\n",
    "   Therefore, the jth component of $\\Delta w_i$ is given by\n",
    "   <center> $\\Delta w_{ij} = \\eta f(w^T_ia)a_j = \\eta s_i a_j$        where $j = 1, 2, ..., M $ </center> <br />\n",
    "   where $s_i$ is the output signal of the ith unit.\n",
    "   The function chosen here is a hard limiting function such as binary function which is defined as follows: <br />\n",
    "    <center>$f(x) = 1, x > 0$ <br />\n",
    "    $f(x) = -1, x <=0$</center>\n",
    "4. Training: We present one training example to the neuron randomly instead of batch processing. At each iteration we normalize the weight vector by dividing the weight vector by its $l2-norm$ i.e., <br/>\n",
    "    <center>$w = \\frac{w}{||w||_2}$</center><br />\n",
    "    Moreover, the learning rate $\\eta$ is close to 1 and the weights are initialized to random values close to zero (using Gaussian Density with mean = 0 and standard deviation = 0.1) <br />\n",
    "5. Testing: For testing we will use two inputs to the gate and consider all the possible combinations of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanSquaredError(X, y, w):\n",
    "    '''\n",
    "        This function calculates the mean squared error over all\n",
    "        training examples.\n",
    "    '''\n",
    "    activation = np.dot(X, w)                 # activation value\n",
    "    y_pred = np.where(activation > 0, 1, -1)  # output value\n",
    "    \n",
    "    return np.sum((y_pred - y)**2)/X.shape[0] # Error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HebbianLearning(X, y, learning_rate=1.0, norm=2):\n",
    "    '''\n",
    "        Function which implements the Hebbian Learning Law.\n",
    "    '''\n",
    "    \n",
    "    # Augmenting the X matrix with 1 for the bias term\n",
    "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    # Randomly initilizing weight close to 0\n",
    "    w = np.random.normal(loc=0.0, scale=0.1, size=X.shape[1])\n",
    "    \n",
    "    error = []\n",
    "    \n",
    "    # Loop until stopping criterion is met\n",
    "    while True:\n",
    "        # Generating random index for random training example\n",
    "        i = np.random.randint(low=0, high=4, size=1)[0]\n",
    "        \n",
    "        activation = np.dot(X[i, :], w)      # Activation value\n",
    "        s = np.where(activation > 0, -1, 1)       # Output value\n",
    "\n",
    "        w = w + learning_rate*s*X[i, :].T    # Weight update\n",
    "\n",
    "        w = w/np.linalg.norm(w, ord=norm)              # Normalizing the weight vector\n",
    "\n",
    "        error.append(MeanSquaredError(X, y, w))        # error calculation\n",
    "\n",
    "        # early stopping\n",
    "        if MeanSquaredError(X, y, w) == 0:\n",
    "            break\n",
    "            \n",
    "    return error, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train vector\n",
    "X = np.array([[-1, -1],\n",
    "             [-1, 1],\n",
    "             [1, -1],\n",
    "             [1, 1]])\n",
    "\n",
    "y_and = np.array([-1, -1, -1, 1])        # Train y vector for AND\n",
    "y_nand = np.array([1, 1, 1, -1])         # Train y vector for NAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations until convergence for AND gate: 186\n",
      "Number of iterations until convergence for NAND gate: 3975\n"
     ]
    }
   ],
   "source": [
    "error_and, w_and = HebbianLearning(X, y_and, learning_rate=1.0, norm=2)    # Training AND gate\n",
    "error_nand, w_nand = HebbianLearning(X, y_nand, learning_rate=1.0, norm=2) # Training NAND gate\n",
    "\n",
    "print(\"Number of iterations until convergence for AND gate:\", len(error_and))\n",
    "print(\"Number of iterations until convergence for NAND gate:\", len(error_nand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for AND gate:  [-0.6812164   0.06868287  0.72885312]\n",
      "Weights for NAND gate:  [ 0.6887066  -0.72390857 -0.04049192]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights for AND gate: \", w_and)\n",
    "print(\"Weights for NAND gate: \", w_nand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(X_test, result):\n",
    "    '''\n",
    "        This function is to print the result of the learned model.\n",
    "    '''\n",
    "    print(pd.DataFrame({\"a1\": X_test[:, 0], \"a2\": X_test[:, 1], \"s\": result}).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HebbianPredict(X, w):\n",
    "    '''\n",
    "        This function is to predict the outcome for the model\n",
    "        learned by Hebbian Learning Law.\n",
    "    '''\n",
    "    \n",
    "    # Augmenting the X matrix with 1 for the bias term\n",
    "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    activation = np.dot(X, w)              # Activation value\n",
    "    \n",
    "    return np.where(activation > 0, 1, -1) # Output Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Result\n",
      "-------------\n",
      "   a1  a2  s\n",
      "0  -1  -1 -1\n",
      "1  -1   1 -1\n",
      "2   1  -1 -1\n",
      "3   1   1  1\n",
      "\n",
      "NAND Result\n",
      "-------------\n",
      "   a1  a2  s\n",
      "0  -1  -1  1\n",
      "1  -1   1  1\n",
      "2   1  -1  1\n",
      "3   1   1 -1\n"
     ]
    }
   ],
   "source": [
    "# Test vector\n",
    "X = np.array([[-1, -1],\n",
    "             [-1, 1],\n",
    "             [1, -1],\n",
    "             [1, 1]])\n",
    "\n",
    "print(\"AND Result\")\n",
    "print(\"-------------\")\n",
    "print_result(X, HebbianPredict(X, w_and))     # Testing AND gate\n",
    "print(\"\\nNAND Result\")\n",
    "print(\"-------------\")\n",
    "print_result(X, HebbianPredict(X, w_nand))    # Testing NAND gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "**Effect of using different norms to normalize the weights:** <br />\n",
    "\n",
    "**NOTE:** Infinite norm is defined as the $\\max_{i}{a_i}$ where $a_i$ the ith component of the vector $a$.\n",
    "\n",
    "**AND Gate**\n",
    "\n",
    "| Norm | Number of iterations until convergence|\n",
    "| --- | --- |\n",
    "| l1 | 17 |\n",
    "| l2 | 5181 |\n",
    "| infinte | Not converging |\n",
    "\n",
    "**NAND Gate**\n",
    "\n",
    "| Norm | Number of iterations until convergence|\n",
    "| --- | --- |\n",
    "| l1 | 6 |\n",
    "| l2 | 4299 |\n",
    "| infinite | Not converging |\n",
    "\n",
    "**Effect of using different learning rate:** <br />\n",
    "\n",
    "**AND Gate**\n",
    "\n",
    "| Learning rate ($\\eta$) | Number of iterations until convergence |\n",
    "| --- | --- |\n",
    "| 0.1 | 531 |\n",
    "| 1.0 | 2020 |\n",
    "| 10.0 | 3 |\n",
    "\n",
    "**NAND Gate**\n",
    "\n",
    "| Learning rate ($\\eta$) | Number of iterations until convergence |\n",
    "| --- | --- |\n",
    "| 0.1 | 1085 |\n",
    "| 1.0 | 1069 |\n",
    "| 10.0 | 7 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "* The Hebbian Learning Law is able to learn appropriate weights in order to implement the AND and NAND gate.\n",
    "* Hebbian Law strengthens the weight if the input and output are of the same sign.\n",
    "* As we increase the order of the norm used for normalizing the weights, the number of iterations until convergence increases.\n",
    "* As we change the learning rate ($\\eta$), we donot observe any pattern in the number of iterations until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-2: Correlation Learning Law\n",
    "#### Hypothesis: \n",
    "The Correlation Learning Law should be able to learn appropriate weights to implement the AND gate and NAND gate. <br />\n",
    "\n",
    "#### Truth Tables:\n",
    "\n",
    "**AND Gate** <br />\n",
    "\n",
    "| $X$ | $Y$ | $Output$ | \n",
    "| --- | --- | --- |\n",
    "| $0$ | $0$ | $0$ |\n",
    "| $0$ | $1$ | $0$ |\n",
    "| $1$ | $0$ | $0$ |\n",
    "| $1$ | $1$ | $1$ |\n",
    "\n",
    "**NAND Gate** <br />\n",
    "\n",
    "| $X$ | $Y$ | $Output$ | \n",
    "| --- | --- | --- |\n",
    "| $0$ | $0$ | $1$ |\n",
    "| $0$ | $1$ | $1$ |\n",
    "| $1$ | $0$ | $1$ |\n",
    "| $1$ | $1$ | $0$ |\n",
    "\n",
    "#### Experimental Description:\n",
    "1.   Data generation: For training purposes we will consider all the possible combinations of two inputs to the gate along with their desired output i.e. the truth table will be our training data. However, the values of $0$ in the output will be replaced by $-1$. <br />\n",
    "2.  Objective: We should be able to realize the following:<br />\n",
    "\n",
    "**AND Gate** <br />\n",
    "\n",
    "| $a_1$ | $a_2$ | $b$ | \n",
    "| --- | --- | --- |\n",
    "| $-1$ | $-1$ | $-1$ |\n",
    "| $-1$ | $1$ | $-1$ |\n",
    "| $1$ | $-1$ | $-1$ |\n",
    "| $1$ | $1$ | $1$ |\n",
    "\n",
    "**NAND Gate** <br />\n",
    "\n",
    "| $a_1$ | $a_2$ | $b$ | \n",
    "| --- | --- | --- |\n",
    "| $-1$ | $-1$ | $1$ |\n",
    "| $-1$ | $1$ | $1$ |\n",
    "| $1$ | $-1$ | $1$ |\n",
    "| $1$ | $1$ | $-1$ |\n",
    "\n",
    "3. Operations: <br />\n",
    "   The change in the weight vector is given by:\n",
    "   <center> $\\Delta w_i = \\eta b_ia$ </center> <br />\n",
    "   Therefore, the jth component of $\\Delta w_i$ is given by\n",
    "   <center> $\\Delta w_{ij} = \\eta b_i a_j$        where $j = 1, 2, ..., M $ </center> <br />\n",
    "   where $s_i$ is the output signal of the ith unit.\n",
    "   The function chosen here is a hard limiting function such as binary function which is defined as follows: <br />\n",
    "    <center>$f(x) = 1, x > 0$ <br />\n",
    "    $f(x) = -1, x <=0$</center>\n",
    "4. Training: We present one training example to the neuron randomly instead of batch processing. At each iteration we normalize the weight vector by dividing the weight vector by its $l2-norm$ i.e., <br/>\n",
    "    <center>$w = \\frac{w}{||w||_2}$</center><br />\n",
    "    Moreover, the learning rate $\\eta$ is close to 1 and the weights are initialized to random values close to zero (using Gaussian Density with mean = 0 and standard deviation = 0.1) <br />\n",
    "5. Testing: For testing we will use two inputs to the gate and consider all the possible combinations of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrelationLearning(X, y, learning_rate=1.0, norm=2):\n",
    "    '''\n",
    "        Function which implements the Hebbian Learning Law.\n",
    "    '''\n",
    "    \n",
    "    # Augmenting the X matrix with 1 for the bias term\n",
    "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    # Randomly initilizing weight close to 0\n",
    "    w = np.random.normal(loc=0.0, scale=0.1, size=X.shape[1])\n",
    "    \n",
    "    error = []\n",
    "    \n",
    "    # Iterating over training examples\n",
    "    for i in range(X.shape[0]):\n",
    "        w = w + learning_rate*y[i]*X[i, :].T           # Weight update\n",
    "\n",
    "        w = w/np.linalg.norm(w, ord=norm)              # Normalizing the weight vector\n",
    "\n",
    "        error.append(MeanSquaredError(X, y, w))        # Error calculation\n",
    "\n",
    "        # early stopping\n",
    "        if MeanSquaredError(X, y, w) == 0:\n",
    "            break\n",
    "            \n",
    "    return error, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations until convergence for AND gate: 1\n",
      "Number of iterations until convergence for NAND gate: 1\n"
     ]
    }
   ],
   "source": [
    "error_and, w_and = CorrelationLearning(X, y_and, learning_rate=1.0, norm=2)    # Training AND gate\n",
    "error_nand, w_nand = CorrelationLearning(X, y_nand, learning_rate=1.0, norm=2) # Training NAND gate\n",
    "\n",
    "print(\"Number of iterations until convergence for AND gate:\", len(error_and))\n",
    "print(\"Number of iterations until convergence for NAND gate:\", len(error_nand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for AND gate:  [-0.60067426  0.42863088  0.67488221]\n",
      "Weights for NAND gate:  [ 0.59784196 -0.58895713 -0.54379636]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights for AND gate: \", w_and)\n",
    "print(\"Weights for NAND gate: \", w_nand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrelationPredict(X, w):\n",
    "    '''\n",
    "        This function is to predict the outcome for the model\n",
    "        learned by Correlation Learning Law.\n",
    "    '''\n",
    "    \n",
    "    # Augmenting the X matrix with 1 for the bias term\n",
    "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    activation = np.dot(X, w)               # Activation Value\n",
    "    \n",
    "    return np.where(activation > 0, 1, -1)  # Output Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Result\n",
      "-------------\n",
      "   a1  a2  s\n",
      "0  -1  -1 -1\n",
      "1  -1   1 -1\n",
      "2   1  -1 -1\n",
      "3   1   1  1\n",
      "\n",
      "NAND Result\n",
      "-------------\n",
      "   a1  a2  s\n",
      "0  -1  -1  1\n",
      "1  -1   1  1\n",
      "2   1  -1  1\n",
      "3   1   1 -1\n"
     ]
    }
   ],
   "source": [
    "# Test vector\n",
    "X = np.array([[-1, -1],\n",
    "             [-1, 1],\n",
    "             [1, -1],\n",
    "             [1, 1]])\n",
    "\n",
    "print(\"AND Result\")\n",
    "print(\"-------------\")\n",
    "print_result(X, CorrelationPredict(X, w_and))\n",
    "print(\"\\nNAND Result\")\n",
    "print(\"-------------\")\n",
    "print_result(X, CorrelationPredict(X, w_nand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "**Effect of using different learning rate:** <br />\n",
    "\n",
    "**AND Gate**\n",
    "\n",
    "| Learning rate ($\\eta$) | Convergence |\n",
    "| --- | --- |\n",
    "| 0.01 | Not reaching to the solution |\n",
    "| 0.1 | Not reaching to the solution |\n",
    "| 1.0 | Reached Solution |\n",
    "| 1.5 | Reached Solution |\n",
    "\n",
    "**NAND Gate**\n",
    "\n",
    "| Learning rate ($\\eta$) | Convergence |\n",
    "| --- | --- |\n",
    "| 0.01 | Not reaching to the solution |\n",
    "| 0.1 | Not reaching to the solution |\n",
    "| 1.0 | Reached Solution |\n",
    "| 1.5 | Reached Solution |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "* The Correlation Learning Law is able to learn appropriate weights in order to implement the AND and NAND gate.\n",
    "* The Correlation Learning Law updates the weight according to the correlation between input and desired output.\n",
    "* We observe that only if keep the learning rate ($\\eta$) close to 1, then only the model converges to the optimal solution.\n",
    "* The number of iterations until convergence is significantly less than the Hebbian Learning Law. This can be attributed to fact that Hebbian Law is unsupervised and Correlation Law is supervised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Title**: Demonstrate the realization of XOR gate using NAND gates learned by using different learning laws for a neuron.\n",
    "\n",
    "**Objective**: To observe and understand the working of different learning laws for a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-1: Hebbian Learning Law\n",
    "#### Hypothesis: \n",
    "The NAND gate learned using Hebbian Learning law should be able to implement XOR gate. <br />\n",
    "\n",
    "#### Truth Tables:\n",
    "\n",
    "**XOR Gate** <br />\n",
    "\n",
    "| $X$ | $Y$ | $Output$ | \n",
    "| --- | --- | --- |\n",
    "| $0$ | $0$ | $0$ |\n",
    "| $0$ | $1$ | $1$ |\n",
    "| $1$ | $0$ | $1$ |\n",
    "| $1$ | $1$ | $0$ |\n",
    "\n",
    "#### Experimental Description:\n",
    "1.  Objective: We should be able to realize the following:<br />\n",
    "\n",
    "**XOR Gate** <br />\n",
    "\n",
    "| $a_1$ | $a_2$ | $b$ | \n",
    "| --- | --- | --- |\n",
    "| $-1$ | $-1$ | $-1$ |\n",
    "| $-1$ | $1$ | $1$ |\n",
    "| $1$ | $-1$ | $1$ |\n",
    "| $1$ | $1$ | $-1$ |\n",
    "\n",
    "2. Operations: <br />\n",
    "   We would combine the learned NAND gates in the following order:\n",
    "   ![NAND_implementation_of_XOR](tmp/NAND_XOR.png)\n",
    "   <center> Source: https://en.wikipedia.org/wiki/NAND_logic </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HebbianXOR(X, w):\n",
    "    '''\n",
    "        This function implements the XOR gate using the NAND gate\n",
    "        learned using the Hebbian Learning Law.\n",
    "    '''\n",
    "    \n",
    "    # Result for the 1st NAND gate in the circuit\n",
    "    nand1 = HebbianPredict(X, w)\n",
    "    \n",
    "    # Result for the 2nd NAND gate in the circuit\n",
    "    nand2 = HebbianPredict(np.c_[X[:, 0], nand1], w)\n",
    "    \n",
    "    # Result for the 3rd NAND gate in the circuit\n",
    "    nand3 = HebbianPredict(np.c_[X[:, 1], nand1], w)\n",
    "    \n",
    "    # Result for the 4th NAND gate in the circuit\n",
    "    nand4 = HebbianPredict(np.c_[nand2, nand3], w)\n",
    "    \n",
    "    # printing the output\n",
    "    print_result(X, nand4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a1  a2  s\n",
      "0  -1  -1 -1\n",
      "1  -1   1  1\n",
      "2   1  -1  1\n",
      "3   1   1 -1\n"
     ]
    }
   ],
   "source": [
    "# Test vector\n",
    "X = np.array([[-1, -1],\n",
    "             [-1, 1],\n",
    "             [1, -1],\n",
    "             [1, 1]])\n",
    "\n",
    "HebbianXOR(X, w_nand) # Testing XOR gate using Hebbian Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-2: Correlation Learning Law\n",
    "#### Hypothesis: \n",
    "The NAND gate learned using correlation Learning law should be able to implement XOR gate. <br />\n",
    "\n",
    "#### Truth Tables:\n",
    "\n",
    "**XOR Gate** <br />\n",
    "\n",
    "| $X$ | $Y$ | $Output$ | \n",
    "| --- | --- | --- |\n",
    "| $0$ | $0$ | $0$ |\n",
    "| $0$ | $1$ | $1$ |\n",
    "| $1$ | $0$ | $1$ |\n",
    "| $1$ | $1$ | $0$ |\n",
    "\n",
    "#### Experimental Description:\n",
    "1.  Objective: We should be able to realize the following:<br />\n",
    "\n",
    "**XOR Gate** <br />\n",
    "\n",
    "| $a_1$ | $a_2$ | $b$ | \n",
    "| --- | --- | --- |\n",
    "| $-1$ | $-1$ | $-1$ |\n",
    "| $-1$ | $1$ | $1$ |\n",
    "| $1$ | $-1$ | $1$ |\n",
    "| $1$ | $1$ | $-1$ |\n",
    "\n",
    "2. Operations: <br />\n",
    "   We would combine the learned NAND gates in the following order:\n",
    "   ![NAND_implementation_of_XOR](tmp/NAND_XOR.png)\n",
    "   <center> Source: https://en.wikipedia.org/wiki/NAND_logic </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrelationXOR(X, w):\n",
    "    '''\n",
    "        This function implements the XOR gate using the NAND gate\n",
    "        learned using the Correlation Learning Law.\n",
    "    '''\n",
    "    \n",
    "    # Result for the 1st NAND gate in the circuit\n",
    "    nand1 = CorrelationPredict(X, w)\n",
    "    \n",
    "    # Result for the 2nd NAND gate in the circuit\n",
    "    nand2 = CorrelationPredict(np.c_[X[:, 0], nand1], w)\n",
    "    \n",
    "    # Result for the 3rd NAND gate in the circuit\n",
    "    nand3 = CorrelationPredict(np.c_[X[:, 1], nand1], w)\n",
    "    \n",
    "    # Result for the 4th NAND gate in the circuit\n",
    "    nand4 = CorrelationPredict(np.c_[nand2, nand3], w)\n",
    "    \n",
    "    # printing the output\n",
    "    print_result(X, nand4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a1  a2  s\n",
      "0  -1  -1 -1\n",
      "1  -1   1  1\n",
      "2   1  -1  1\n",
      "3   1   1 -1\n"
     ]
    }
   ],
   "source": [
    "# Test vector\n",
    "X = np.array([[-1, -1],\n",
    "             [-1, 1],\n",
    "             [1, -1],\n",
    "             [1, 1]])\n",
    "\n",
    "CorrelationXOR(X, w_nand) # Testing XOR gate using Correlation Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations and Conclusion\n",
    "* By combining learned NAND gates we are able to implement the XOR gates. This means that we can solve non-linearly separable classification problem by using multi-layer neural networks with non-linear processing units. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Artificial Neural Networks by B. Yegnanarayana, 1999\n",
    "* https://www.geeksforgeeks.org/hebbian-learning-rule-with-implementation-of-and-gate/\n",
    "* Pictures taken from https://en.wikipedia.org/wiki/NAND_logic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
